{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "POSSIBLE_INPUT_CHARS = f'{\"\".join(list(map(str, range(10))))}{string.ascii_lowercase}-'\n",
    "POSSIBLE_OUTPUT_CHARS = f'{\"\".join(list(map(str, range(10))))}-'\n",
    "CURRENT_DAY = datetime.utcnow()\n",
    "ROW_COUNT = 14000\n",
    "\n",
    "\n",
    "def string_to_ids(s: str, chars: str) -> list[int]:\n",
    "    ids = []\n",
    "\n",
    "    for char in s.lower():\n",
    "        idx = chars.index(char)\n",
    "\n",
    "        ids.append(idx)\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "def shuffle(vals: tf.RaggedTensor, targets: tf.RaggedTensor) -> (tf.RaggedTensor, tf.RaggedTensor):\n",
    "    a = tf.random.shuffle(tf.range(vals.shape[0]))\n",
    "    b = tf.reshape(a, (vals.shape[0], 1))\n",
    "    shuffled_vals = tf.gather_nd(vals, b)\n",
    "    shuffled_targets = tf.gather_nd(targets, b)\n",
    "\n",
    "    return shuffled_vals, shuffled_targets\n",
    "\n",
    "\n",
    "def pad_year(year: int) -> str:\n",
    "    return f'{\"\".join(map(str, [0] * (4 - len(str(year)))))}{year}'\n",
    "\n",
    "\n",
    "def get_date_pairs() -> (np.ndarray, np.ndarray):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    all_years = np.arange(ROW_COUNT).tolist()\n",
    "    years_padded = np.array([pad_year(year) for year in all_years])\n",
    "\n",
    "    np.random.shuffle(years_padded)\n",
    "\n",
    "    for counter in range(ROW_COUNT):\n",
    "        date = CURRENT_DAY - timedelta(days=counter)\n",
    "        year_month_day = date.strftime('%Y-%m-%d')\n",
    "        year_month_name_day = date.strftime('%Y-%B-%d')\n",
    "        _, month_name, day = year_month_name_day.split('-')\n",
    "        year = years_padded[counter]\n",
    "        year_month_day = f'{year}-{year_month_day[5:]}'\n",
    "        xs.append(tf.constant(\n",
    "            string_to_ids(f'{year}-', POSSIBLE_INPUT_CHARS) +\n",
    "            string_to_ids(f'{month_name}-', POSSIBLE_INPUT_CHARS) +\n",
    "            string_to_ids(day, POSSIBLE_INPUT_CHARS)))\n",
    "        ys.append(tf.constant(string_to_ids(year_month_day, POSSIBLE_OUTPUT_CHARS)))\n",
    "\n",
    "    ragged_xs = tf.ragged.stack(xs, axis=0)\n",
    "    ragged_ys = tf.ragged.stack(ys, axis=0)\n",
    "\n",
    "    return shuffle(ragged_xs, ragged_ys)\n",
    "\n",
    "\n",
    "X, y = get_date_pairs()\n",
    "X = (X + 1).to_tensor()\n",
    "y = y.to_tensor()\n",
    "seventy_percent_count = int(X.shape[0] * .7)\n",
    "ninety_percent_count = int(X.shape[0] * .9)\n",
    "X_train, y_train = X[:seventy_percent_count, :], y[:seventy_percent_count, :]\n",
    "X_valid, y_valid = X[seventy_percent_count:ninety_percent_count, :], y[seventy_percent_count:ninety_percent_count, :]\n",
    "X_test, y_test = X[ninety_percent_count:, :], y[ninety_percent_count:, :]\n",
    "max_output_length = y.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "sos_id = len(POSSIBLE_OUTPUT_CHARS) + 1\n",
    "\n",
    "\n",
    "def shifted_output_sequences(y: tf.Tensor) -> tf.Tensor:\n",
    "    sos_tokens = tf.fill(dims=(len(y), 1), value=sos_id)\n",
    "\n",
    "    return tf.concat([sos_tokens, y[:, :-1]], axis=1)\n",
    "\n",
    "\n",
    "X_train_decoder = shifted_output_sequences(y_train)\n",
    "X_valid_decoder = shifted_output_sequences(y_valid)\n",
    "X_test_decoder = shifted_output_sequences(y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training part - LSTM variant"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def scheduler(drop_after: int) -> Callable[[int, int], float]:\n",
    "    def drop(epoch: int, learning_rate: int) -> float:\n",
    "        if epoch < drop_after:\n",
    "            return learning_rate\n",
    "        else:\n",
    "            return learning_rate * tf.math.exp(-0.2)\n",
    "\n",
    "    return drop\n",
    "\n",
    "\n",
    "def get_callbacks() -> (keras.callbacks.EarlyStopping, keras.callbacks.ModelCheckpoint, keras.callbacks.TensorBoard):\n",
    "    the_name = 'encoder_decoder_w_teacher_forcing'\n",
    "    patience = 5\n",
    "    model_dir = os.path.join(os.curdir, 'saved_models')\n",
    "    run_logdir_root = os.path.join(os.curdir, 'tensor_logs')\n",
    "    dirs = [\n",
    "        name\n",
    "        for name in os.listdir(run_logdir_root)\n",
    "        if os.path.isdir(os.path.join(run_logdir_root, name)) and name.startswith(name)\n",
    "    ]\n",
    "    dirs_count = len(dirs) + 1\n",
    "    run_logdir = os.path.join(run_logdir_root, f'{the_name}_{dirs_count}')\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, min_delta=1e-4)\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(model_dir, f'{the_name}_{dirs_count}.h5'), save_best_only=True)\n",
    "    tensorboard = keras.callbacks.TensorBoard(run_logdir, histogram_freq=1, profile_batch=10)\n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler(10))\n",
    "\n",
    "    return early_stopping, model_checkpoint, tensorboard, lr_scheduler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "307/307 [==============================] - 11s 16ms/step - loss: 1.3919 - accuracy: 0.4831 - val_loss: 1.0255 - val_accuracy: 0.6108 - lr: 0.0100\n",
      "Epoch 2/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 0.6311 - accuracy: 0.7562 - val_loss: 0.2749 - val_accuracy: 0.8830 - lr: 0.0100\n",
      "Epoch 3/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 0.1080 - accuracy: 0.9631 - val_loss: 0.0178 - val_accuracy: 0.9974 - lr: 0.0100\n",
      "Epoch 4/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 0.0115 - accuracy: 0.9983 - val_loss: 0.0081 - val_accuracy: 0.9987 - lr: 0.0100\n",
      "Epoch 5/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 0.0037 - accuracy: 0.9996 - val_loss: 0.0048 - val_accuracy: 0.9993 - lr: 0.0100\n",
      "Epoch 6/40\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 0.0160 - accuracy: 0.9959 - val_loss: 0.0096 - val_accuracy: 0.9977 - lr: 0.0100\n",
      "Epoch 7/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0026 - val_accuracy: 0.9995 - lr: 0.0100\n",
      "Epoch 8/40\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.0062 - val_accuracy: 0.9986 - lr: 0.0100\n",
      "Epoch 9/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0043 - val_accuracy: 0.9989 - lr: 0.0100\n",
      "Epoch 10/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 0.0088 - accuracy: 0.9978 - val_loss: 0.0071 - val_accuracy: 0.9982 - lr: 0.0100\n",
      "Epoch 11/40\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 4.7065e-04 - val_accuracy: 1.0000 - lr: 0.0082\n",
      "Epoch 12/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 2.6820e-04 - accuracy: 1.0000 - val_loss: 3.0033e-04 - val_accuracy: 1.0000 - lr: 0.0067\n",
      "Epoch 13/40\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 1.8395e-04 - accuracy: 1.0000 - val_loss: 2.3547e-04 - val_accuracy: 1.0000 - lr: 0.0055\n",
      "Epoch 14/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 1.4551e-04 - accuracy: 1.0000 - val_loss: 2.0104e-04 - val_accuracy: 1.0000 - lr: 0.0045\n",
      "Epoch 15/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 1.2305e-04 - accuracy: 1.0000 - val_loss: 1.7668e-04 - val_accuracy: 1.0000 - lr: 0.0037\n",
      "Epoch 16/40\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 1.0793e-04 - accuracy: 1.0000 - val_loss: 1.5939e-04 - val_accuracy: 1.0000 - lr: 0.0030\n",
      "Epoch 17/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 9.6634e-05 - accuracy: 1.0000 - val_loss: 1.4568e-04 - val_accuracy: 1.0000 - lr: 0.0025\n",
      "Epoch 18/40\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 8.7946e-05 - accuracy: 1.0000 - val_loss: 1.3471e-04 - val_accuracy: 1.0000 - lr: 0.0020\n",
      "Epoch 19/40\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 8.1040e-05 - accuracy: 1.0000 - val_loss: 1.2610e-04 - val_accuracy: 1.0000 - lr: 0.0017\n",
      "Epoch 20/40\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 7.5463e-05 - accuracy: 1.0000 - val_loss: 1.1941e-04 - val_accuracy: 1.0000 - lr: 0.0014\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "latent_dim = 128\n",
    "encoder_input_lstm = keras.Input(shape=(None,))\n",
    "encoder_embedding_lstm = keras.layers.Embedding(input_dim=len(POSSIBLE_INPUT_CHARS) + 1,\n",
    "                                                output_dim=embedding_size)(encoder_input_lstm)\n",
    "encoder_lstm = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "_, encoder_state_h_lstm, encoder_state_c_lstm = encoder_lstm(encoder_embedding_lstm)\n",
    "encoder_states_lstm = [encoder_state_h_lstm, encoder_state_c_lstm]\n",
    "\n",
    "decoder_input_lstm = keras.Input(shape=(None,))\n",
    "decoder_embedding_lstm = keras.layers.Embedding(input_dim=len(POSSIBLE_OUTPUT_CHARS) + 2,\n",
    "                                                output_dim=embedding_size)(decoder_input_lstm)\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True)\n",
    "decoder_lstm_output = decoder_lstm(decoder_embedding_lstm, initial_state=encoder_states_lstm)\n",
    "decoder_dense_lstm = keras.layers.Dense(len(POSSIBLE_OUTPUT_CHARS) + 1, activation='softmax')\n",
    "decoder_outputs_lstm = decoder_dense_lstm(decoder_lstm_output)\n",
    "model_lstm = keras.Model(inputs=[encoder_input_lstm, decoder_input_lstm], outputs=[decoder_outputs_lstm])\n",
    "adam_opt = keras.optimizers.Adam(learning_rate=.01)\n",
    "\n",
    "model_lstm.compile(optimizer=adam_opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping, model_checkpoint, tensorboard, lr_scheduler = get_callbacks()\n",
    "history_lstm = model_lstm.fit(\n",
    "    [X_train, X_train_decoder],\n",
    "    y_train,\n",
    "    epochs=40,\n",
    "    validation_data=([X_valid, X_valid_decoder], y_valid),\n",
    "    callbacks=[early_stopping, model_checkpoint, tensorboard, lr_scheduler])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training part - GRU variant"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "307/307 [==============================] - 7s 14ms/step - loss: 1.1078 - accuracy: 0.5799 - val_loss: 0.4972 - val_accuracy: 0.8091 - lr: 0.0100\n",
      "Epoch 2/40\n",
      "307/307 [==============================] - 4s 11ms/step - loss: 0.2769 - accuracy: 0.8990 - val_loss: 0.1520 - val_accuracy: 0.9413 - lr: 0.0100\n",
      "Epoch 3/40\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 0.0888 - accuracy: 0.9662 - val_loss: 0.0186 - val_accuracy: 0.9959 - lr: 0.0100\n",
      "Epoch 4/40\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 0.0154 - accuracy: 0.9963 - val_loss: 0.0185 - val_accuracy: 0.9956 - lr: 0.0100\n",
      "Epoch 5/40\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 9.9919e-04 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "Epoch 6/40\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 4.3447e-04 - accuracy: 1.0000 - val_loss: 4.1498e-04 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "Epoch 7/40\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 2.1185e-04 - accuracy: 1.0000 - val_loss: 2.8078e-04 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "Epoch 8/40\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 1.4939e-04 - accuracy: 1.0000 - val_loss: 2.0961e-04 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "Epoch 9/40\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 1.1335e-04 - accuracy: 1.0000 - val_loss: 1.6822e-04 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "Epoch 10/40\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 8.9134e-05 - accuracy: 1.0000 - val_loss: 1.3663e-04 - val_accuracy: 1.0000 - lr: 0.0100\n",
      "Epoch 11/40\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 7.2334e-05 - accuracy: 1.0000 - val_loss: 1.1642e-04 - val_accuracy: 1.0000 - lr: 0.0082\n",
      "Epoch 12/40\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 6.1416e-05 - accuracy: 1.0000 - val_loss: 1.0319e-04 - val_accuracy: 1.0000 - lr: 0.0067\n",
      "Epoch 13/40\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 5.3692e-05 - accuracy: 1.0000 - val_loss: 9.3511e-05 - val_accuracy: 1.0000 - lr: 0.0055\n",
      "Epoch 14/40\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 4.7999e-05 - accuracy: 1.0000 - val_loss: 8.4701e-05 - val_accuracy: 1.0000 - lr: 0.0045\n"
     ]
    }
   ],
   "source": [
    "encoder_input_gru = keras.Input(shape=(None,))\n",
    "encoder_embedding_gru = keras.layers.Embedding(input_dim=len(POSSIBLE_INPUT_CHARS) + 1,\n",
    "                                               output_dim=embedding_size)(encoder_input_gru)\n",
    "encoder_gru = keras.layers.GRU(latent_dim, return_state=True)\n",
    "_, encoder_state_h_gru = encoder_gru(encoder_embedding_gru)\n",
    "\n",
    "decoder_input_gru = keras.Input(shape=(None,))\n",
    "decoder_embedding_gru = keras.layers.Embedding(input_dim=len(POSSIBLE_OUTPUT_CHARS) + 2,\n",
    "                                               output_dim=embedding_size)(decoder_input_gru)\n",
    "decoder_gru = keras.layers.GRU(latent_dim, return_sequences=True)\n",
    "decoder_gru_output = decoder_gru(decoder_embedding_gru, initial_state=encoder_state_h_gru)\n",
    "decoder_dense_gru = keras.layers.Dense(len(POSSIBLE_OUTPUT_CHARS) + 1, activation='softmax')\n",
    "decoder_outputs_gru = decoder_dense_gru(decoder_gru_output)\n",
    "model_gru = keras.Model(inputs=[encoder_input_gru, decoder_input_gru], outputs=[decoder_outputs_gru])\n",
    "adam_opt = keras.optimizers.Adam(learning_rate=.01)\n",
    "\n",
    "model_gru.compile(optimizer=adam_opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping, model_checkpoint, tensorboard, lr_scheduler = get_callbacks()\n",
    "history = model_gru.fit(\n",
    "    [X_train, X_train_decoder],\n",
    "    y_train,\n",
    "    epochs=40,\n",
    "    validation_data=([X_valid, X_valid_decoder], y_valid),\n",
    "    callbacks=[early_stopping, model_checkpoint, tensorboard, lr_scheduler])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference part with LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "['1999-05-01', '8123-06-30', '1213-07-16', '6990-12-12', '5432-09-09']"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_date_strs(date_strs: list[str], chars=POSSIBLE_INPUT_CHARS) -> tf.Tensor:\n",
    "    X_ids = [string_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "\n",
    "    return (X + 1).to_tensor()\n",
    "\n",
    "\n",
    "def prepare_date_strs_padded(date_strs: list[str]) -> tf.Tensor:\n",
    "    ids = prepare_date_strs(date_strs)\n",
    "\n",
    "    if ids.shape[1] < X_train.shape[1]:\n",
    "        ids = tf.pad(ids, [[0, 0], [0, X_train.shape[1] - ids.shape[1]]])\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "def ids_to_date_strs(ids: list[int], chars: str = POSSIBLE_OUTPUT_CHARS) -> list[str]:\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence]) for sequence in ids]\n",
    "\n",
    "\n",
    "def predict_date_strs(date_strs: list[str]) -> list[str]:\n",
    "    X_new = prepare_date_strs_padded(date_strs)\n",
    "    y_pred = tf.fill(dims=(len(X_new), 1), value=sos_id)\n",
    "\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - y_pred.shape[1]\n",
    "        X_decoder = tf.pad(y_pred, [[0, 0], [0, pad_size]])\n",
    "        y_probas_next = model_lstm.predict([X_new, X_decoder])[:, index:index+1]\n",
    "        y_pred_next = tf.argmax(y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        y_pred = tf.concat([y_pred, y_pred_next], axis=1)\n",
    "\n",
    "    return ids_to_date_strs(y_pred[:, 1:-1] + 1)\n",
    "\n",
    "\n",
    "predict_date_strs(['1999-May-01', '8123-June-30', '1213-July-16', '6990-Demember-12', '5432-Sepxxmber-09'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Maybe it's not able to predict dates with [1, 3] or [5, ...) numbers on the year position, but it can translate to the correct date even with a misspelling (sometimes :) )!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
